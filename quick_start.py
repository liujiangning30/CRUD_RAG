import os
os.environ['OPENAI_API_KEY'] = 'Your api key'

import argparse
import pandas as pd
from loguru import logger
from dataclasses import dataclass, field

from src.datasets.xinhua import get_task_datasets
from evaluator import BaseEvaluator
from src.llms import GPT
from src.llms import Qwen_7B_Chat
from src.llms import InternLMClient, GPTBatched, InternLM
from src.tasks.summary import Summary
from src.tasks.continue_writing import ContinueWriting
from src.tasks.hallucinated_modified import HalluModified
from src.tasks.quest_answer import QuestAnswer1Doc, QuestAnswer2Docs, QuestAnswer3Docs
from src.retrievers import BaseRetriever, CustomBM25Retriever, EnsembleRetriever, EnsembleRerankRetriever
from src.embeddings.base import HuggingfaceEmbeddings
from src.util import convert_to_table


@dataclass
class MorkRetriever():
    collection_name: str = None
    similarity_top_k: int = 1


def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


parser = argparse.ArgumentParser()

# Model related options
parser.add_argument('--model_name', default='internlm2_5-7b-chat', help="Name of the model to use, such as internlm2_5-7b-chat, gpt-4o-mini, gpt-4o-2024-08-06")
parser.add_argument('--model_type', default='InternLM', help="Which llm class to use, such as GPTBatched, InternLM")
parser.add_argument('--model_path', default='/cpfs02/llm/shared/public/zhaoqian/ckpt/7B/240623/P-volc_internlm2_5_boost1_7B_FT_merge_boost_bbh_v2', help="Path of the model")
parser.add_argument('--tp', type=int, default=1)
parser.add_argument('--temperature', type=float, default=0.8, help="Controls the randomness of the model's text generation")
parser.add_argument('--max_new_tokens', type=int, default=1024, help="Maximum number of new tokens to be generated by the model")
parser.add_argument('--batch_size', type=int, default=16, help="batch size for inferencing")

# Dataset related options
parser.add_argument('--data_path', default='data/crud_split/split_merged.json', help="Path to the dataset")
parser.add_argument('--shuffle', type=bool, default=True, help="Whether to shuffle the dataset")
parser.add_argument('--save_infer_output', type=bool, default=True, help="Whether to save the infer output to resume for an abnormal termination")
parser.add_argument('--output_dir', type=str, default='./output', help="Output dir")

parser.add_argument('--embedding_name', default='/cpfs01/user/liujiangning/.cache/huggingface/hub/models--BAAI--bge-base-zh-v1.5/snapshots/f03589ceff5aac7111bd60cfc7d497ca17ecac65')
parser.add_argument('--embedding_dim', type=int, default=768)

# Index related options
parser.add_argument('--docs_path', default='data/80000_docs', help="Path to the retrieval documents")
parser.add_argument('--docs_type', default="txt", help="Type of the documents")
parser.add_argument('--chunk_size', type=int, default=128, help="Chunk size")
parser.add_argument('--chunk_overlap', type=int, default=0, help="Overlap chunk size")
parser.add_argument('--construct_index', action='store_true', default=False, help="Whether to construct an index")
parser.add_argument('--add_index', action='store_true', default=False, help="Whether to add an index")
parser.add_argument('--collection_name', default="docs_80k_chuncksize_128_0", help="Name of the collection")

# Retriever related options
parser.add_argument('--mork_retriever', type=bool, default=True, help="Mork a fake retriever")
parser.add_argument('--retrieve_top_k', type=int, default=8, help="Top k documents to retrieve")
parser.add_argument('--retriever_name', default="base", help="Name of the retriever")
parser.add_argument('--save_retrieved_ctx', default=False, help="Whether to save retrieved contexts")
parser.add_argument('--use_retrieved_ctx', default=True, help="Whether to use retrieved contexts")
parser.add_argument('--use_gt_ctx', type=str2bool, default=True, help="Whether to use ground-truth paragraph as contexts")
parser.add_argument('--inject_negative_ctx', type=str2bool, default=True, help="Whether to inject negative paragraphs to contexts")

# Metric related options
parser.add_argument('--do_eval', action='store_false', default=False, help="Whether to do evaluation")
parser.add_argument('--quest_eval', action='store_true', default=True, help="Whether to use QA metrics(RAGQuestEval)")
parser.add_argument('--quest_eval_model', type=str, default='gpt-4o-mini', help="Which model to use for RAGQuestEval")
parser.add_argument('--quest_eval_type', default='QuestEvalGPTBatched', help="Which questEval client to use, such as QuestEvalAPI, QuestEvalGPTBatched")
parser.add_argument('--quest_eval_model_url', default=None, help="Url of the model to request, such as http://22.8.17.203:23333")
parser.add_argument('--bert_score_eval', action='store_true', help="Whether to use bert_score metrics")

# Evaluation related options
# cost time 4.5 hours for all task.
parser.add_argument('--task', default='hallu_modified', help="Task to perform")
parser.add_argument('--re_quest_eval', action='store_true', default=False, help="Whether to rerun RAGQuestEval")
parser.add_argument('--num_threads', type=int, default=20, help="Number of threads")
parser.add_argument('--show_progress_bar', action='store', default=True, type=bool, help="Whether to show a progress bar")
parser.add_argument('--contain_original_data', action='store_true', help="Whether to contain original data")

args = parser.parse_args()
logger.info(args)

if args.model_path:
    llm = eval(args.model_type)(model_name=args.model_name, model_path=args.model_path, tp=args.tp, temperature=args.temperature, max_new_tokens=args.max_new_tokens)
else:
    llm = eval(args.model_type)(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens)

embed_model = HuggingfaceEmbeddings(model_name=args.embedding_name)

if args.mork_retriever:
    retriever = MorkRetriever()
    retriever.collection_name = args.collection_name
    retriever.similarity_top_k = args.retrieve_top_k
else:
    if args.retriever_name == "base":
        retriever = BaseRetriever(
            args.docs_path, embed_model=embed_model, embed_dim=args.embedding_dim,
            chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap,
            construct_index=args.construct_index, add_index=args.add_index,
            collection_name=args.collection_name, similarity_top_k=args.retrieve_top_k
        )
    elif args.retriever_name == "bm25":
        retriever = CustomBM25Retriever(
            args.docs_path, embed_model=embed_model, chunk_size=args.chunk_size, 
            construct_index=args.construct_index,
            chunk_overlap=args.chunk_overlap, similarity_top_k=args.retrieve_top_k
        )
    elif args.retriever_name == "hybrid":
        retriever = EnsembleRetriever(
            args.docs_path, embed_model=embed_model, embed_dim=args.embedding_dim,
            chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap,
            construct_index=args.construct_index, add_index=args.add_index,
            collection_name=args.collection_name, similarity_top_k=args.retrieve_top_k
        )
    elif args.retriever_name == "hybrid-rerank":
        retriever = EnsembleRerankRetriever(
            args.docs_path, embed_model=embed_model, embed_dim=args.embedding_dim,
            chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap,
            construct_index=args.construct_index, add_index=args.add_index,
            collection_name=args.collection_name, similarity_top_k=args.retrieve_top_k
        )
    else:
        raise ValueError(f"Unknown retriever: {args.retriever_name}")

task_mapping = {
    'event_summary': [Summary],
    'continuing_writing': [ContinueWriting],
    'hallu_modified': [HalluModified],
    'quest_answer': [QuestAnswer1Doc, QuestAnswer2Docs, QuestAnswer3Docs],
    'questanswer_1doc': [QuestAnswer1Doc],
    'questanswer_2docs': [QuestAnswer2Docs],
    'questanswer_3docs': [QuestAnswer3Docs],
    'all': [Summary, ContinueWriting, HalluModified, QuestAnswer1Doc, QuestAnswer2Docs, QuestAnswer3Docs]
}

alias_task_mapping = {
    "ContinueWriting": "continuing_writing",
    "HalluModified": "hallu_modified",
    "QuestAnswer1Doc": "questanswer_1doc",
    "QuestAnswer2Docs": "questanswer_2docs",
    "QuestAnswer3Docs": "questanswer_3docs",
    "Summary": "event_summary",
}

if args.task in alias_task_mapping:
    args.task = alias_task_mapping[args.task]

if args.task not in task_mapping:
    raise ValueError(f"Unknown task: {args.task}")

tasks = []
for task in task_mapping[args.task]:
    task_inst = task(
        use_quest_eval=args.quest_eval,
        use_bert_score=args.bert_score_eval,
        quest_eval_model=args.quest_eval_model,
        quest_eval_model_url=args.quest_eval_model_url,
        quest_eval_type=args.quest_eval_type)
    tasks.append(task_inst)

datasets = get_task_datasets(args.data_path, args.task, w_ctx=args.use_retrieved_ctx)
overall_results = dict()
for task, dataset in zip(tasks, datasets):
    print(f"processing task: {task.__class__.__name__}")
    evaluator = BaseEvaluator(
        task,
        llm,
        retriever,
        dataset,
        save_context=args.save_retrieved_ctx,
        data_path=args.data_path,
        output_dir=args.output_dir,
        save_infer_output=args.save_infer_output,
        use_gt_ctx=args.use_gt_ctx,
        inject_negative_ctx=args.inject_negative_ctx,
        num_threads=args.num_threads)
    result = evaluator.run(show_progress_bar=args.show_progress_bar, contain_original_data=args.contain_original_data, batch_size=args.batch_size, do_eval=args.do_eval, re_quest_eval=args.re_quest_eval)
    if args.do_eval:
        task_metrics = convert_to_table(result, task.__class__.__name__)
        overall_results[task.__class__.__name__] = task_metrics

if args.do_eval:
    output_tasks = ['ContinueWriting', 'HalluModified', 'QuestAnswer1Doc', 'QuestAnswer2Docs', 'QuestAnswer3Docs', 'Summary']
    ordered_overall_results = []
    for task_name in output_tasks:
        if task_name in overall_results:
            ordered_overall_results.extend(overall_results[task_name])
    df = pd.DataFrame(ordered_overall_results, columns=['task', 'metric', 'value'])
    df.to_csv(f'output/overall_results_{args.model_name}_quest_eval_{args.quest_eval_model}.csv', index=False)